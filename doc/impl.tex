% -*-LaTeX-*-

\section{Implementation}
\label{sec:impl}

%
% Outline of the implementation
%    - back storage
%    - libpfs
%    - pfsd + FUSE
% Why we implemented pFS as a library
%   easy to use API for working with our versionning system
%   independant of the communication layer used to propagate updates
% Layout of the back storage
%   ~/.pfs/data
%   ~/.pfs/info
%   ~/.pfs/groups
%   groups information (participating sd) contained in pFS itself
%   so that it gets propagated
% libpfs
%    layout of the exposed file system
%    pfs_instance -> link to back storage
%                    opened files
%    POSIX API
%       what happens :
%       path lookup -> need for caching
%       when file modified link (old_id, new_id) + update vv +
%                        pfs_set_entry
%            relinking + file close -> happen on back storage
%            update vv + pfs_set_entry happen in memory
%       pfs_sync_cache
%    updt as argument of set_entry
%    pfs_set_updt_cb
%    pfs_apply_updt
%    => layer above libpfs has access to POSIX I/F, receive updt
%        struct as callbacks for propagation and must apply locally
%        the updts it receive + putting the ressource in data/
% pfsd
%    couple of line FUSE stub
%    in charge of update propagation : need for a log
%    our prototype using IP
%       Need for a naming service (DNS ?)
%       Need for relay servers (Devices being firewall)
%       transmitting updates : simple transmission of pfs_updt struct
%       transmitting data : simple data transmission, LBFS/rsync
%                           should be used
%    prototype using Bluetooth
%       no naming service / relay : service exported and devices
%       attached
%   LAN transmission priority
%   Future work : need for transmission layer decision


We implemented pFS as a two level architecture based on the device
local file system (Figure \ref{PfsImpl})~:

\begin {itemize}
\item \textbf{Back storage}~: a directory on the local file system
  containing all data and metadata used and generated by pFS.
\item \textbf{libpfs}~: the core functionalities of pFS related to
  versionning are implemented in libpfs which exports a POSIX file
  system interface along with extra functionalities specific to
  pFS.
\item \textbf{Fuse stub}~: exposes pFS to the user as a file system
  using Fuse~\cite{henk:fuse}. It translates directly the calls
  received from Fuse to the POSIX file system interface exported by
  libpfs.
\item \textbf{pfsd}~: a daemon that is responsible for propagating
  updates using any communication channel available and apply updates
  received from other $sd$s. pfsd receives callbacks from libpfs
  whenever an update is made locally, and uses libpfs API to commit
  updates received from remote $sd$s on the local device.
\end {itemize}

\begin{figure}[ht]
\begin{center}
  \includegraphics [scale=0.6] {img/impl}
  \caption{\label{PfsImpl} {\small pFS Implementation as a two level
      architecture based on the device local file system}}
\end{center}
\end{figure}

We designed the core component of pFS as a library in order to provide
an easy-to-use API for taking advantage of our versionning system, that
is independent of the communication channels used to propagate the
updates between the different $sd$s.

\subsection {Back Storage}

The back storage is layed out as follows~: \\ 
{\tt /.pfs/data/} \\ 
{\tt /.pfs/dir/} \\ 
{\tt /.pfs/info} \\ 
{\tt /.pfs/groups} \\

{\tt /.pfs/info} is a file containing the user's unique id, the device's
unique id, along with the user name and the device name. {\tt
  /.pfs/groups} contains the list of the groups the device is involved
in, with for each group the unique id of the {\tt pfs\_dir} structure
representing the root of the group tree. {\tt /.pfs/dir/} contains one
file per {\tt pfs\_dir} structure existing in the system, named after
the {\tt pfs\_dir} unique id. It contains a serialized version of the
{\tt pfs\_dir} structure used by the versionning system (Figure
\ref{MemStruct}). As we described in section \ref{sec:vers}
entries in {\tt pfs\_dir} are versionned and each version contains a
type and an unique id. If the type of the version is $DIR$, the id
refers to a file in {\tt /.pfs/dir/}. Otherwise, if the type of the
version is $FIL$, the id refers to a file in {\tt /.pfs/data/},
containing the actual content of the file.

\subsection {libpfs}

The pFS file system exposed to the user through Fuse is layed out as
follows~: the root directory contains a list of directories
representing each group in which the device is involved. The initial
group on any device is {\tt me} and involves all the devices that
belongs to the owner of that device.

Since libpfs is structured as a library, all functions get a {\tt
  pfs\_instance} structure as argument. This structure contains the path
of the back storage and the list of currently opened files. libpfs
exports a POSIX-like file system API~: {\tt pfs\_open}, {\tt
  pfs\_close}, {\tt pfs\_pwrite}, {\tt pfs\_unlink}... These calls get
absolute paths (containing the group directory) as argument for files
and directories, mainly for compliance with Fuse API
which uses absolute paths from the root of the mounted file system.

Therefore, we need to walk down the chain of {\tt pfs\_dir} structures
anytime an operation has to be made on a file that has no associated
file handler. Since {\tt pfs\_dir} structures are stored in files
contained in {\tt /.pfs/dir/} we have implemented a cache to walk
theses structures in memory. This avoids the overhead of having to
open and read as many files in {\tt /.pfs/dir/} as the depth of the
path we are given as argument. The cache is a simple LRU cache
implemented with a hash table mapping {\tt pfs\_dir} ids to their
actual in-memory data structures. It does not need to be large since
modifications made inside a directory results in many requests to the
same {\tt pfs\_dir} structures. In our implementation we are caching
64 {\tt pfs\_dir} structures, a pointer to the cache is stored in the
{\tt pfs\_instance} structure. libpfs provides a {\tt
  pfs\_sync\_cache} function for periodically writing back the cache
to disk (every 10s in our implementation of pfsd). The cache has
incurred an important improvement over the performances of libpfs
since {\tt pfs\_dir} fetchs and write backs are amortized over a lot
of operations in case of intensive workloads, or appear as
asynchronous for less intensive workloads.

\paragraph{Local file operations~:}
When a file is created, a new unique id is generated, the associated
file is created in {\tt /.pfs/data/} and a {\tt pfs\_set\_entry} call
is issued to insert the new {\tt pfs\_ver} into its parent {\tt
  pfs\_dir} structure. The file creation happens on the back storage
file system while the version update happens in memory using the cache
we described. Opening a file for reading does not need to update any
versioning information and is directly mapped to an {\tt open} call on
the back storage file. Therefore creating a file or opening a file for
reading using libpfs incurs almost no overhead.

At the contrary, opening a file for writing necessitates the
generation of a new unique id. This new mapping is needed as soon as
the {\tt pfs\_open} call is issued since the actual id for the file
might be revoked if an update containing a dominating version is
received while the file is edited. The {\tt pfs\_ver} associated with
the newly generated id is inserted in memory when the file is
subsequently closed. Therefore opening a file for writing incurs a non
negligeable overhead and maps to two calls on the back storage~: {\tt
  link (old\_id, new\_id)} and {\tt open (new\_id)}.

Merging a version into the local main version is translated into a
call to {\tt pfs\_set\_entry}, and an {\tt unlink} call to the back
storage to revoke the version that has been superseded.

\paragraph{Local directory operations~:}
Directory creations are mapped to the creation of a file in {\tt
  /.pfs/dir/}, the insertion of a {\tt pfs\_dir} structure in the
cache and the update of the parent {\tt pfs\_dir}. Directory removals
are mapped to an {\tt unlink} call to the back storage, the removal of
an entry in the cache and the update of the parent {\tt
  pfs\_dir}. Therefore, directory operations incurs a slight overhead
by mapping directory operations to back storage files operations and
in-memory updates.

\paragraph{}

libpfs also provides interfaces used for two-way communication of
updates with pfsd. Functions can be registered to be called
any time a {\tt pfs\_set\_entry} is issued. The argument passed to
those callbacks is a {\tt pfs\_updt} structure that contains all the
arguments needed to issue the same {\tt pfs\_set\_entry} on another
device~:

\begin{figure}[ht]
\begin{center}
{\tt \small
\begin{verbatim}
struct pfs_updt
{
  char grp_id [PFS_ID_LEN];
  char dir_id [PFS_ID_LEN];  
  char name [PFS_NAME_LEN];
  uint8_t reclaim;
  struct pfs_ver * ver;
};
\end{verbatim}
}
\end{center}
\caption{\label{MemStruct}
{\small {\tt pfs\_updt} structure propagated by pfsd among the
    different instances of libpfs.}}
\end{figure}

The updates received by pfsd from the network can be applied to the
local copy of pFS by calling {\tt pfs\_set\_entry} directly.

Finally, libpfs exports a {\tt pfs\_create\_group} function for
creating a new group. The function creates a new directory {\tt
  /new\_group} on the root of the pFS tree, and generates a new {\tt
  pfs\_dir} structure representing the root of {\tt
  new\_group}. libpfs looks for the list of $sd$s participating in any
given group in the file {\tt /new\_group/.pfs\_sd}. Adding and
removing $sd$s is delegated to pfsd. The file {\tt .pfs\_sd} simply
contains a mapping from $sd$s ids to $sd$s names used for naming files
when multiple versions co-exist.

\subsection {pfsd}

We have implemented a prototype of pfsd using IP as a transport layer
that we will refer to by prot\_pfsd. We used a simple protocol allowing
to send updates from one device to another and retransmit associated
files contained in {\tt /.pfs/data}. To allow the devices to discover
each others and retrieve their respective IPs, we designed a central
naming service to which they register when connected to the network.

prot\_pfsd does not optimize back storage file transfers~: the entire
content of a modified file is retransmitted as its id is
regenerated. This is clearly a mechanism that can be optimized as
stated in section \ref{subsec:depfsd}.

prot\_pfsd does not provide yet automated management of groups. The
$sd$s participating in a group are specified at creation and
prot\_pfsd does not allow dynamic addition of $sd$ after creation.

prot\_pfsd keeps a log of the updates it receives from the local
instance of libpfs and from other devices. The log entries are {\tt
  pfs\_updt} structures augmented with a propagation vector. Updates
to this propagation vector is retransmitted between the $sd$s until it
is equal to 1. When it is the case, the log entry is deleted to avoid
wasting storage capacity.

We paid attention in our design to make the daemon resilient to brutal
disconnections that frequently occurs on laptop computers. The
flexibility provided by the structure of the updates helped a lot in
this case, since updates transmissions are not dependent on each
others in the context of libpfs. After two instances of prot\_pfsd get
disconnected, when connectivity is retrived, prot\_pfsd simply needs
to continue its sequential propagation of the updates stored in the
log.

\subsection {Design of a pFS daemon}
\label {subsec:depfsd}

There is no final design for pfsd since it is highly dependent on
which type of device and which type of communication channel it relies
on. This section is a survey of lessons we have learnt from designing
prot\_pfsd and directives we have to iterate on our implementation.

\paragraph {The role of pfsd~:}
\begin{itemize}
\item propagate to other devices the updates generated locally.
\item receive update from other devices and apply them to the local
  instance of libpfs.
\item group management~: $sd$s addition and removal
\end{itemize}

\paragraph {Naming service}
In the context of IP a naming service is necessary to enable the
devices to discover each others' address from their unique ids. We
used a central server, but DNS could be used as a naming service. A
naming service is not always needed, As an example, a daemon using
Bluetooth would rely on the built-in mechanisms to attach devices such
that they recognize each others when placed at proximity.

\paragraph {Update Logging~:}
pfsd need to maintain a log of every update it sees in order to
retransmit them to other devices. Three important points have to be
considered when implementing update logging~:
\begin{itemize}
\item The callbacks used by libpfs to transmit local updates to pfsd are
  placed on the critical execution path of any local
  operation. Therefore, when received, updates should be temporarely
  stored in memory before being written back asyncrhonously to permanent
  storage.
\item Depending on the context, updates should be augmented by pfsd
  with a propagation vector to keep track of which devices have
  already receive any given update.
\item Any update that has been propagated to all devices can and
  should be removed from the log to avoid wasting local storage
  capacity. This forbids the simple retransmission of the whole update
  log when a new $sd$ joins a group. Nevertheless, this log can easily
  be regenerated from the state stored by libpfs on the back storage.
\end{itemize}

\paragraph {File transmission~:}
Along with the updates, back storage files stored in {\tt /.pfs/data/}
have to be transmitted between the devices. Files associated with
different version of a same file are very likely to be
similar. rsync\cite{tridgell:rsync} principles as well as concepts
introduced in LBFS\cite{muthitacharoen:lbfs} should definitely be
considered for file transmission in order to avoid wasting bandwidth.

\paragraph {Group management~:}
Group management is a real challenge in pFS. libpfs preconizes that
the list of $sd$s participating in a group is stored in the pFS group
tree itself. This allows propagation of up-to-date group information
using the pFS mechanism itself. Any communication between two $sd$s
should start by a preambule to check that the destination $sd$ already
know the existence of the group to be updated. If not, a special
command should be issued to instruct the destination pfsd to create the group
locally. Then, an initial synchronization done through the propagation
of the whole log of update for that group would  have to take place. A $sd$
removal can be done simply by updating the {\tt .pfs\_sd} file. The $sd$
removed would then stop receiving updates from the group.

Even if not needed for correctness, it would be great if the $id$ of
the removed $sd$ was erased from any version vector existing in the
system. This can be done by removing the $sd$ from versions where it
not the last updater and moving versions where it is the last updater
to a specific directory similar to the {\tt lost+found} directory
commonly used for file system checking. Even if it is not the case
yet, this functionality should be provided by libpfs and the trigger
for this action propagated as an update by pfsd.

\paragraph {Choice of communication channel~:}
If different communication channels are available (IP (LAN / WAN),
Bluetooth, USB), it is essential to define a policy to decide which
layer to use given the updates to be propagated. It would be
inefficient to propagate GB of music over WAN from a desktop computer
at home to a desktop computer at work if this data can be efficiently
pushed to a mobile device such as a digital player that will be able
to quickly retransmit it when placed at proximity of the desktop
computer at work. There is still a lot of work to be done on the
design of such policy as described in \ref{sec:futwk}.


% Local Variables:
% tex-main-file: "main.ltx"
% tex-command: "make;:"
% tex-dvi-view-command: "make preview;:"
% End:
