% -*-LaTeX-*-

\section{Implementation}
\label{sec:impl}

%
% Outline of the implementation
%    - back storage
%    - libpfs
%    - pfsd + FUSE
% Why we implemented pFS as a library
%   easy to use API for working with our versionning system
%   independant of the communication layer used to propagate updates
% Layout of the back storage
%   ~/.pfs/data
%   ~/.pfs/info
%   ~/.pfs/groups
%   groups information (participating sd) contained in pFS itself
%   so that it gets propagated
% libpfs
%    layout of the exposed file system
%    pfs_instance -> link to back storage
%                    opened files
%    POSIX API
%       what happens :
%       path lookup -> need for caching
%       when file modified link (old_id, new_id) + update vv +
%                        pfs_set_entry
%            relinking + file close -> happen on back storage
%            update vv + pfs_set_entry happen in memory
%       pfs_sync_cache
%    updt as argument of set_entry
%    pfs_set_updt_cb
%    pfs_apply_updt
%    => layer above libpfs has access to POSIX I/F, receive updt
%        struct as callbacks for propagation and must apply locally
%        the updts it receive + putting the ressource in data/
% pfsd
%    couple of line FUSE stub
%    in charge of update propagation : need for a log
%    our prototype using IP
%       Need for a naming service (DNS ?)
%       Need for relay servers (Devices being firewall)
%       transmitting updates : simple transmission of pfs_updt struct
%       transmitting data : simple data transmission, LBFS/rsync
%                           should be used
%    prototype using Bluetooth
%       no naming service / relay : service exported and devices
%       attached
%   LAN transmission priority
%   Future work : need for transmission layer decision


We implemented pFS as a two level architecture based on the device
local file system (Figure \ref{PfsImpl}):

\begin {itemize}
\item \textbf{Back-end storage}: a directory on the local file system
  containing all data and metadata used and generated by pFS.
\item \textbf{libpfs}: the core functionalitiy of pFS related to
  versioning is implemented in libpfs which exports a POSIX file
  system interface along with extra functionality specific to
  pFS.
\item \textbf{Fuse stub}: exposes pFS to the user as a file system
  using Fuse~\cite{henk:fuse}. It translates directly the calls
  received from Fuse to the POSIX file system interface exported by
  libpfs.
\item \textbf{pfsd}: a daemon that is responsible for propagating
  updates using any communication channel available and applying
  updates received from other $sd$s. pfsd receives callbacks from
  libpfs whenever an update is made locally, and uses libpfs API to
  commit updates received from remote $sd$s on the local device.
\end {itemize}

\begin{figure}[ht]
\begin{center}
  \includegraphics [scale=0.8] {impl_overw}
  \caption{\label{PfsImpl} {\small pFS Implementation as a two level
      architecture based on the device local file system}}
\end{center}
\end{figure}

We designed the core component of pFS as a library in order to provide
an easy-to-use API for taking advantage of our versionning system, that
is independent of the communication channels used to propagate the
updates between the different $sd$s.

\subsection {Back-End Storage}

The back-end storage is layed out as follows: \\ 
{\tt /.pfs/data/} \\ 
{\tt /.pfs/dir/} \\ 
{\tt /.pfs/info} \\ 
{\tt /.pfs/groups} \\

{\tt /.pfs/info} is a file containing the user's unique id, the device's
unique id, along with the user name and the device name. {\tt
  /.pfs/groups} contains the list of the groups the device is involved
in, with for each group the unique id of the {\tt pfs\_dir} structure
representing the root of the group tree. {\tt /.pfs/dir/} contains one
file per {\tt pfs\_dir} structure existing in the system, named after
the {\tt pfs\_dir} unique id. It contains a serialized version of the
{\tt pfs\_dir} structure used by the versionning system (Figure
\ref{MemStruct}). As we described in section \ref{sec:vers}
entries in {\tt pfs\_dir} are versionned and each version contains a
type and an unique id. If the type of the version is $DIR$, the id
refers to a file in {\tt /.pfs/dir/}. Otherwise, if the type of the
version is $FIL$, the id refers to a file in {\tt /.pfs/data/},
containing the actual content of the file.

\subsection {libpfs}

The pFS file system exposed to the user through Fuse is layed out as
follows: the root directory contains a list of directories
representing each group in which the device is involved. The initial
group on any device is {\tt me} and involves all the devices that
belongs to the owner of that device.

Since libpfs is structured as a library, all functions get a {\tt
  pfs\_instance} structure as argument. This structure contains the path
of the back-end storage and the list of currently opened files. libpfs
exports a POSIX-like file system API: {\tt pfs\_open}, {\tt
  pfs\_close}, {\tt pfs\_pwrite}, {\tt pfs\_unlink}\ldots These calls get
absolute paths (containing the group directory) as argument for files
and directories, mainly for compability with the Fuse API
which uses absolute paths from the root of the mounted file system.

Therefore, we need to walk down the chain of {\tt pfs\_dir} structures
anytime an operation has to be made on a file that has no associated
file handler. Since {\tt pfs\_dir} structures are stored in files
contained in {\tt /.pfs/dir/} we have implemented a cache to walk
theses structures in memory. This avoids the overhead of having to
open and read as many files in {\tt /.pfs/dir/} as the depth of the
path we are given as argument. The cache is a simple LRU cache
implemented with a hash table mapping {\tt pfs\_dir} ids to their
actual in-memory data structures. It does not need to be large since
modifications made inside a directory results in many requests to the
same {\tt pfs\_dir} structures. In our implementation we are caching
64 {\tt pfs\_dir} structures, a pointer to the cache is stored in the
{\tt pfs\_instance} structure. libpfs provides a {\tt
  pfs\_sync\_cache} function for periodically writing back the cache
to disk (every 10s in our implementation of pfsd). The cache has
incurred an important improvement over the performances of libpfs.

\paragraph{Local file operations:}
When a file is created, a new unique id is generated, the associated
file is created in {\tt /.pfs/data/} and a {\tt pfs\_set\_entry} call
is issued to insert the new {\tt pfs\_ver} into its parent {\tt
  pfs\_dir} structure. The file creation happens on the back-end storage
file system while the version update happens in memory on the cached
{\tt pfs\_dir} structure. Opening a file for reading does not need to
update any versioning information and is directly mapped to an {\tt
  open} call on the back-end storage file. 

Opening a file for writing incurs more overhead since it necessitates the
generation of a new unique id. This new mapping is needed as soon as
the {\tt pfs\_open} call is issued since the actual id for the file
might be revoked if an update containing a dominating version is
received while the file is edited. The {\tt pfs\_ver} associated with
the newly generated id is inserted in memory when the file is
subsequently closed. Therefore opening a file for writing incurs a non
negligeable overhead and maps to two calls on the back-end storage: a
{\tt link} and an {\tt open} operation.

Merging a version into the local main version is translated into a
call to {\tt pfs\_set\_entry}, and an {\tt unlink} call to the back
storage to revoke the version that has been superseded.

Finally, an {\tt fsync} call flushes the cache to disk and maps to the
associated {\tt fsync} call on the back-end storage file.

\paragraph{Local directory operations:}
Directory creations are mapped to the creation of a file in {\tt
  /.pfs/dir/}, the insertion of a {\tt pfs\_dir} structure in the
cache and the update of the parent {\tt pfs\_dir}. Directory removals
are mapped to an {\tt unlink} call to the back-end storage, the removal of
an entry in the cache and the update of the parent {\tt
  pfs\_dir}. Therefore, directory operations incur a slight overhead
by mapping directory operations to back-end storage files operations and
in-memory updates.


\paragraph{}

libpfs also provides interfaces used for two-way communication of
updates with pfsd. Functions can be registered to be called any time a
{\tt pfs\_set\_entry} is issued. The argument passed to those
callbacks is a {\tt pfs\_updt} structure (Figure \ref{PfsUpdt}) that
contains all the arguments needed to issue the same {\tt
  pfs\_set\_entry} on another device. The updates received by pfsd
from the network can be applied to the local copy of pFS by calling
{\tt pfs\_set\_entry} directly.

\begin{figure}[ht]
\begin{center}
{\tt \small
\begin{verbatim}
struct pfs_updt
{
  char grp_id [PFS_ID_LEN];
  char dir_id [PFS_ID_LEN];  
  char name [PFS_NAME_LEN];
  uint8_t reclaim;
  struct pfs_ver * ver;
};
\end{verbatim}
}
\end{center}
\caption{\label{PfsUpdt}
{\small {\tt pfs\_updt} structure propagated by pfsd among the
    different instances of libpfs.}}
\end{figure}

Finally, libpfs exports a {\tt pfs\_create\_group} function for
creating a new group. The function creates a new directory {\tt
  /new\_group} on the root of the pFS tree, and generates a new {\tt
  pfs\_dir} structure representing the root of {\tt
  new\_group}. libpfs looks for the list of $sd$s participating in any
given group in the file {\tt /new\_group/.pfs\_sd}. Adding and
removing $sd$s is delegated to pfsd. The file {\tt .pfs\_sd} simply
has to contain a mapping from $sd$s ids to $sd$s names used for naming files
when multiple versions co-exist.

\subsection {pfsd}

We have implemented a prototype of pfsd using IP as a transport layer
that we will refer to by prot\_pfsd. We used a simple protocol
allowing the system to send updates from one device to another and
retransmit associated files contained in {\tt /.pfs/data}. To allow
the devices to discover each other and retrieve their respective IPs,
we designed a central naming service with which they register when
connected to the network.

prot\_pfsd still lacks some functionality : It does not optimize
back-end storage file transfers: the entire content of a modified file
is retransmitted as its id is regenerated. This is clearly a mechanism
that can be optimized as stated in section \ref{subsec:depfsd}. It
does not provide automated management of groups. The $sd$s
participating in a group are specified at creation: dynamic addition of
$sd$s is not yet supported.

prot\_pfsd keeps a log of the updates it receives from the local
instance of libpfs and from other devices. The log entries are {\tt
  pfs\_updt} structures augmented with a propagation vector. Updates
to this propagation vector are retransmitted between the $sd$s until it
is equal to vector 1. When this is the case, the log entry is deleted to
avoid wasting storage capacity. prot\_pfsd opportunistically attempts
to propagate any unpropagated updates between two $sd$s when the
connection status change from \emph{disconnected} to \emph{connected}
between those two $sd$s.

We paid attention in our design to make the daemon resilient to brutal
disconnections that frequently occur on laptop computers. The
flexibility provided by the structure of the updates helped a lot in
this case, since update transmissions are not dependent on each
others in the context of libpfs. After two instances of prot\_pfsd get
disconnected, when connectivity is revived, prot\_pfsd simply has
to continue its sequential propagation of the updates stored in the
log.

Despite its lack of functionality, prot\_pfsd successfuly showed
the validity and ease of use of libpfs' versioning system, and already 
allowed us to experience the benefits of the personal cloud model we
advocate.

\subsection {What's next \ldots}
\label {subsec:depfsd}

There is no final design for pfsd since it is highly dependent on
which type of device and which type of communication channel it relies
on. This section is a survey of lessons we have learned from designing
prot\_pfsd and directives we have to iterate on our implementation.

\paragraph {The role of pfsd:}
\begin{itemize}
\item propagate to other devices the updates generated locally.
\item receive updates from other devices and apply them to the local
  instance of libpfs.
\item group management: addition and removal of $sd$s
\end{itemize}

\paragraph {Naming service}
In the context of IP a naming service is necessary to enable the
devices to discover each others' address from their unique ids. We
used a central server, but DNS could be used as a naming service. A
naming service is not always needed. As an example, a daemon using
Bluetooth would rely on the built-in mechanisms to attach devices such
that they recognize each other when placed at proximity.

\paragraph {Update Logging:}
pfsd need to maintain a log of every update it sees in order to
retransmit them to other devices. Three important points have to be
considered when implementing update logging:
\begin{itemize}
\item The callbacks used by libpfs to transmit local updates to pfsd are
  placed on the critical execution path of any local
  operation. Therefore, when received, updates should be temporarely
  stored in memory before being written back asyncrhonously to permanent
  storage.
\item Depending on the context, updates should be augmented by pfsd
  with a propagation vector to keep track of which devices have
  already receive any given update.
\item Any update that has been propagated to all devices can and
  should be removed from the log to avoid wasting local storage
  capacity. This forbids the simple retransmission of the whole update
  log when a new $sd$ joins a group. Nevertheless, this log can easily
  be regenerated from the state stored by libpfs on the back-end storage.
\end{itemize}

\paragraph {File transmission:}
Along with the updates, back-end storage files stored in {\tt /.pfs/data/}
have to be transmitted between the devices. Files associated with
different version of a same file are very likely to be
similar. rsync\cite{tridgell:rsync} principles as well as concepts
introduced in LBFS\cite{muthitacharoen:lbfs} should definitely be
considered for file transmission in order to avoid wasting bandwidth.

\paragraph {Group management:}
Group management is a real challenge in pFS. libpfs preconizes that
the list of $sd$s participating in a group is stored in the pFS group
tree itself. This allows propagation of up-to-date group information
using the pFS mechanism itself. Any communication between two $sd$s
should start by a preambule to check that the destination $sd$ already
know the existence of the group to be updated. If not, a special
command should be issued to instruct the destination pfsd to create the group
locally. Then, an initial synchronization done through the propagation
of the whole log of update for that group would  have to take place. A $sd$
removal can be done simply by updating the {\tt .pfs\_sd} file. The $sd$
removed would then stop receiving updates from the group.

Even if not needed for correctness, it would be great if the $id$ of
the removed $sd$ was erased from any version vector existing in the
system. This can be done by removing the $sd$ from versions where it
not the last updater and moving versions where it is the last updater
to a specific directory similar to the {\tt lost+found} directory
commonly used for file system checking. Even if it is not the case
yet, this functionality should be provided by libpfs and the trigger
for this action propagated as an update by pfsd.

\paragraph {Choice of communication channel:}
If different communication channels are available (IP (LAN / WAN),
Bluetooth, USB), it is essential to define a policy to decide which
one to use given the updates to be propagated. It would be
inefficient to propagate GB of music over WAN from a desktop computer
at home to a desktop computer at work if this data can be efficiently
pushed to a mobile device such as a digital player that will be able
to quickly retransmit it when connected to the desktop computer at
work. There is some work to be done on the design of such
policy as described in section \ref{sec:futwk}.


% Local Variables:
% tex-main-file: "main.ltx"
% tex-command: "make;:"
% tex-dvi-view-command: "make preview;:"
% End:
