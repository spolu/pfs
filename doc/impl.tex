% -*-LaTeX-*-

\section{Implementation}
\label{sec:impl}

%
% Outline of the implementation
%    - back storage
%    - libpfs
%    - pfsd + FUSE
% Why we implemented pFS as a library
%   easy to use API for working with our versionning system
%   independant of the communication layer used to propagate updates
% Layout of the back storage
%   ~/.pfs/data
%   ~/.pfs/info
%   ~/.pfs/groups
%   groups information (participating sd) contained in pFS itself
%   so that it gets propagated
% libpfs
%    layout of the exposed file system
%    pfs_instance -> link to back storage
%                    opened files
%    POSIX API
%       what happens :
%       path lookup -> need for caching
%       when file modified link (old_id, new_id) + update vv +
%                        pfs_set_entry
%            relinking + file close -> happen on back storage
%            update vv + pfs_set_entry happen in memory
%       pfs_sync_cache
%    updt as argument of set_entry
%    pfs_set_updt_cb
%    pfs_apply_updt
%    => layer above libpfs has access to POSIX I/F, receive updt
%        struct as callbacks for propagation and must apply locally
%        the updts it receive + putting the ressource in data/
% pfsd
%    couple of line FUSE stub
%    in charge of update propagation : need for a log
%    our prototype using IP
%       Need for a naming service (DNS ?)
%       Need for relay servers (Devices being firewall)
%       transmitting updates : simple transmission of pfs_updt struct
%       transmitting data : simple data transmission, LBFS/rsync
%                           should be used
%    prototype using Bluetooth
%       no naming service / relay : service exported and devices
%       attached
%   LAN transmission priority
%   Future work : need for transmission layer decision

Our implementation of pFS relies on Fuse~\cite{henk:fuse} to expose a
mounted file system to users. All the code runs in user-space and
stores the state of \emph{replicas} in a directory on the device
native file system. We implemented pFS as a two level architecture
(Figure \ref{PfsImpl}):

\begin{figure}[ht]
\begin{center}
  \includegraphics [scale=0.8] {impl_overw}
  \caption{\label{PfsImpl} {\small pFS Implementation as a two level
      architecture based on the device local file system}}
\end{center}
\end{figure}

\begin {itemize}
\item \textbf{Back-end storage}: a directory on the local file system
  containing all data and metadata used and generated by pFS.
\item \textbf{libpfs}: the core functionalitiy of pFS related to
  versioning is implemented in libpfs which exports a POSIX-like file
  system interface along with extra functionality specific to pFS.
\item \textbf{Fuse stub}: exposes pFS to the user as a file system
  using Fuse. It translates directly the calls received from Fuse to
  the libpfs interface.
\item \textbf{pfsd}: a daemon that is responsible for propagating
  updates using any communication channel available and applying
  updates received from other devices. pfsd receives callbacks
  from libpfs whenever an update is made locally, and uses libpfs API
  to commit updates received from remote devices on the local
  \emph{replica}.
\end {itemize}

We designed the core component of pFS as a library in order to provide
an easy-to-use API for taking advantage of our versionning system,
that is independent of the communication channels used to propagate
the updates between the different devices.

%\subsection {Back-End Storage}

%The back-end storage is layed out as follows: \\ 
%{\tt /.pfs/data/} \\ 
%{\tt /.pfs/dir/} \\ 
%{\tt /.pfs/info} \\ 
%{\tt /.pfs/fs} \\

%{\tt /.pfs/info} is a file containing the user's unique id, the
%device's unique id, along with the user name and the device name. {\tt
%  /.pfs/fs} contains the list of the \emph{file systems} the device is
%involved in, with for each \emph{file system} the unique id of the
%{\tt pfs\_dir} structure representing the root of the \emph{replica}
%tree. {\tt /.pfs/dir/} contains one file per {\tt pfs\_dir} structure
%existing in the system, named after the {\tt pfs\_dir} unique id. It
%contains a serialized version of the {\tt pfs\_dir} structure used by
%the versionning system (Figure \ref{MemStruct}). As we described in
%section \ref{sec:vers} entries in {\tt pfs\_dir} are versionned and
%each version contains a type and an unique id. If the type of the
%version is $DIR$, the id refers to a file in {\tt
%  /.pfs/dir/}. Otherwise, if the type of the version is $FIL$, the id
%refers to a file in {\tt /.pfs/data/}, containing the actual content
%of the file.

\subsection {libpfs}

The pFS file system exposed to the user through Fuse is layed
out as follows: the root directory contains a list of directories
representing each \emph{file system} in which the device is
involved. 
%The initial \emph{file system} on any device is {\tt me} and
%involves all the devices that belongs to the owner of that device.

Since libpfs is structured as a library, all functions get a {\tt
  pfs\_instance} structure as argument. This structure contains the
path of the back-end storage and the list of currently opened
files. libpfs exports a POSIX-like file system API: {\tt
  pfs\_open}, {\tt pfs\_close}, {\tt pfs\_pwrite}, {\tt
  pfs\_unlink}\ldots These calls get absolute paths as argument for
files and directories, mainly for compability with the Fuse API which
uses absolute paths from the root of the mounted file system.

Therefore, we need to walk down the chain of {\tt pfs\_dir} structures
anytime an operation has to be made on a file that has no associated
file handler. Since {\tt pfs\_dir} structures are stored in files
contained in the back-end storage, we have implemented a cache to walk
theses structures in memory.  The cache is a simple LRU cache
implemented with a hash table, mapping {\tt pfs\_dir} IDs to their
actual in-memory data structures. It does not need to be large since
modifications made inside a directory result in many requests to the
same {\tt pfs\_dir} structures. In our implementation we are caching
64 {\tt pfs\_dir} structures, a pointer to the cache is stored in the
{\tt pfs\_instance} structure. libpfs provides a {\tt
  pfs\_sync\_cache} function for periodically writing back the cache
to disk (every 10s in our implementation of pfsd). The cache has
incurred an important improvement over the performances of libpfs.

\paragraph{Local file operations:}
When a file is created, a new destination ID is generated. A file named
after this ID is created on the back-end storage and a {\tt pfs\_set\_entry}
call is issued to insert the new {\tt pfs\_ver} into its parent {\tt
  pfs\_dir} structure present or fetched in the cache.  Opening a file
for reading does not need to update any versioning information and is
directly mapped to an {\tt open} call on the back-end storage file.

Opening a file for writing incurs more overhead since it necessitates
the generation of a new destination ID. This new mapping is needed as
soon as the {\tt pfs\_open} call is issued since the actual ID for the
file might be revoked if an update containing a dominating version is
received while the file is edited. The {\tt pfs\_ver} associated with
the newly generated ID is inserted into its parent {\tt pfs\_dir}
structure when the file is subsequently closed. Therefore opening a
file for writing incurs a non negligeable overhead and maps to two
operations on the back-end storage: a {\tt link} and an {\tt open}
call.

Merging a version into the local main version, or deleting a file, is
translated into a call to {\tt pfs\_set\_entry}, and an {\tt unlink}
call on the back-end storage to revoke the version that has been
superseded.

Finally, an {\tt fsync} call flushes the cache to disk and maps to the
associated {\tt fsync} call on the back-end storage file.

\paragraph{Local directory operations:}
Directory creations are mapped to the creation of a file on the
back-end storage, the insertion of a {\tt pfs\_dir} structure in the
cache and the update of the parent {\tt pfs\_dir}. Directory removals
are mapped to an {\tt unlink} call on the back-end storage, the
removal of an entry in the cache and the update of the parent {\tt
  pfs\_dir}. Therefore, directory operations incur a slight overhead
by mapping directory operations to back-end storage files operations.

\paragraph{}
libpfs also provides interfaces used for two-way communication of
updates with pfsd. Functions can be registered by pfsd to be called
any time a {\tt pfs\_set\_entry} is issued. The argument passed to
those callbacks is a {\tt pfs\_updt} structure that contains all the
arguments needed to issue the same {\tt pfs\_set\_entry} on another
device; namely, the ID of the \emph{file system}, the ID of the parent
{\tt pfs\_dir} structure, the name of the file and the new {\tt
  pfs\_ver} structure. The updates received by pfsd from the network
can be applied to the local copy of pFS by calling {\tt
  pfs\_set\_entry} directly.

Finally, libpfs exports a {\tt pfs\_create\_fs} function for creating
a new \emph{file system}. The function creates a new directory {\tt
  /new\_fs} on the root of the pFS tree, and generates a new {\tt
  pfs\_dir} structure representing the root of {\tt new\_fs}. libpfs
looks for the list of \emph{replicas} participating in any given
\emph{file system} in the file {\tt /new\_fs/.pfs\_replica}. Adding
and removing \emph{replicas} is delegated to pfsd.

%The file {\tt .pfs\_sd} simply
%has to contain a mapping from \emph{replicas} ids to 
% \emph{replicas} names used for naming files
%when multiple versions co-exist.

\subsection {pfsd}

We have implemented a prototype of pfsd using IP as a transport layer.
We used a simple protocol allowing the system to send updates from one
device to another and retransmit associated files contents. To allow
the devices to discover each other and retrieve their respective IPs,
we designed a central naming service with which they register when
connected to the network.

pfsd still lacks some functionality; it does not optimize back-end
storage file transfers: the entire content of a modified file is
retransmitted as its ID is regenerated. This is clearly a mechanism
that can be optimized as stated in section \ref{sec:futwk}. pfsd
also does not currently provide automated management of \emph{file
  systems}' \emph{replicas}. The devices participating in a \emph{file
  system} are specified at creation: dynamic addition of
\emph{replicas} is not yet supported. Adding a replica is still
possible by archiving a back-end storage copy of an existing replica,
copying it to the device that has to be added, and modifying the local
configuration files.

pfsd keeps a log of the updates it receives from the local instance of
libpfs and from other devices. The log entries are {\tt pfs\_updt}
structures augmented with a propagation vector. This vector is a list
of booleans representing wether or not the given update has been
propagated to a \emph{replica}. Updates to this propagation vector are
retransmitted between the devices until it is equal to vector 1.  When
this is the case, the log entry is deleted to avoid wasting storage
capacity. Log entries are also elided when their associated versions
are superseded by updates received from the local \emph{replica} or
remote devices.  pfsd opportunistically attempts to propagate any
unpropagated updates between two devices when permitted by the
network.

pfsd has been desgiend to be resilient to brutal
disconnections that frequently occur on laptop computers. The
flexibility provided by the structure of the updates helped a lot in
this case, since update transmissions are not dependent on each others
in the context of libpfs. After two instances of pfsd get
disconnected, when connectivity is revived, pfsd simply restarts its
sequential propagation of the updates stored in its log.

Despite being rudimentary, our prototype of pfsd successfuly
shows the validity and ease of use of libpfs' versioning system, and
has already allowed us to experience the benefits of the personal
cloud model we advocate.

% Local Variables:
% tex-main-file: "main.ltx"
% tex-command: "make;:"
% tex-dvi-view-command: "make preview;:"
% End:
